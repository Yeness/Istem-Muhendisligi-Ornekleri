{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0mUuCpQN7tq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b7ef4b2-3180-4dc4-8005-d29ba6976695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m146.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.6/503.6 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install torch -Uq #LLM'lerin eğitimi, ince ayarı ve tahmin üretim işlemlerini yapmamıza olanak sağlar\n",
        "!pip install transformers -Uq #LLM'ler ile iletişimimizi sağlar\n",
        "!pip install tiktoken -Uq #Metnin kaç token içerdiğini gösterir\n",
        "!pip install datasets -Uq #Veriyi parçalama ve filtreleme gibi işlemlerde kullanılır\n",
        "!pip install matplotlib -Uq #Modelin görsel olarak izlediğimiz kütüphane"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Önceki Ders Kritik"
      ],
      "metadata": {
        "id": "fe_T5Tqz5_yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "cnlXKssk64Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"tokenizer_1.json\", \"r\") as f:\n",
        "  vocab = json.load(f)"
      ],
      "metadata": {
        "id": "GeVVqtp465qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"the capital of the united states is not london\""
      ],
      "metadata": {
        "id": "SkDybiNX6h5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  parts = text.split()\n",
        "  ids = []\n",
        "  for part in parts:\n",
        "    if part in vocab:\n",
        "      value = vocab[part]\n",
        "    else:\n",
        "      value = vocab[\"<unk>\"]\n",
        "    ids.append(value)\n",
        "  return ids"
      ],
      "metadata": {
        "id": "0nvwQuHU6DLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = tokenize(text)"
      ],
      "metadata": {
        "id": "2UTFFNAg6GMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids"
      ],
      "metadata": {
        "id": "YnKXg8Ki7xJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_vocab = {id: part for part, id in vocab.items()}"
      ],
      "metadata": {
        "id": "74XZhfKF77Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detokenize(ids):\n",
        "  text = \"\"\n",
        "  for id in ids:\n",
        "    part = reverse_vocab[id]\n",
        "    text += part + \" \"\n",
        "  text = text.strip()\n",
        "  return text"
      ],
      "metadata": {
        "id": "9UmSJVFE74N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detokenize(token_ids)"
      ],
      "metadata": {
        "id": "brhl95aO78k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yeni Hafta: Subword bazında ayırma"
      ],
      "metadata": {
        "id": "VcLLOjrr8l5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu kod greedy (aç gözlü) yaklaşımı ele alır -> en uzun parçayı seçmeyi garanti eder"
      ],
      "metadata": {
        "id": "-2PRmdH0MEZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self, vocab_path):\n",
        "    with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
        "      self.vocab = json.load(f)                                         # Kelime - ID eşleştirme\n",
        "      self.reverse_vocab = {v: k for k, v in self.vocab.items()}        #\n",
        "\n",
        "  def encode(self, text):\n",
        "    tokens = []\n",
        "\n",
        "    for word in text.split():                                           # Her kelimeyi ayır\n",
        "      i = 0                                                             # Kelimeyi alt parçalara böl\n",
        "      while i < len(word):\n",
        "        found_match = False\n",
        "        for j in range(len(word), i, -1):\n",
        "          subword = word[i:j]\n",
        "          if subword in self.vocab:\n",
        "            tokens.append(self.vocab[subword])\n",
        "            i = j\n",
        "            found_match = True\n",
        "            break\n",
        "        if not found_match:                                            # Eğer kelime sözlükte yoksa\n",
        "          tokens.append(self.vocab.get(\"<unk>\", 0))\n",
        "          i += 1\n",
        "      tokens.append(self.vocab.get(\" \", 1))                            # Kelimelerin sonuna boşluk ekle\n",
        "    if tokens:\n",
        "      tokens.pop()                                                     # Son boşluğu kaldır\n",
        "    return tokens\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \"\"\n",
        "    for id in ids:\n",
        "      text += self.reverse_vocab.get(id, \"<unk>\")                      # Neden önceki decode kısmında boşluk eklerken burada eklemiyoruz\n",
        "    return text"
      ],
      "metadata": {
        "id": "WY3DGoUkVZHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Haftaya Soru"
      ],
      "metadata": {
        "id": "I4UgC4sf_IBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sözlükteki anlamları aynı olabilir. Ya peki anlamsal olarak aynılar mı?"
      ],
      "metadata": {
        "id": "v7TDJQVv_MXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Text_1 = \"The cat chased the dog\"\n",
        "Text_2 = \"The dog chased the dog\""
      ],
      "metadata": {
        "id": "xUDEucDT_X0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Text = \"Hiçbir yüz güzel değil senin yüzünden\""
      ],
      "metadata": {
        "id": "xS_L3kvC_jVF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}