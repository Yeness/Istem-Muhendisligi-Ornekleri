{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch -Uq"
      ],
      "metadata": {
        "id": "TMtz7Fpb0X_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -Uq"
      ],
      "metadata": {
        "id": "QLN6Hr8m0cXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken -Uq"
      ],
      "metadata": {
        "id": "2md39Y3c0cpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -Uq"
      ],
      "metadata": {
        "id": "pH-XeWu-0dQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib -Uq"
      ],
      "metadata": {
        "id": "ze-qSwpN0d2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "class Tokenizer:\n",
        "  def __init__(self, vocab_path):\n",
        "    with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
        "      self.vocab = json.load(f)\n",
        "      self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    tokens = []\n",
        "\n",
        "    for word in text.split():\n",
        "      i = 0\n",
        "      while i < len(word):\n",
        "        found_match = False\n",
        "        for j in range(len(word), i, -1):\n",
        "          subword = word[i:j]\n",
        "          if subword in self.vocab:\n",
        "            tokens.append(self.vocab[subword])\n",
        "            i = j\n",
        "            found_match = True\n",
        "            break\n",
        "        if not found_match:\n",
        "          tokens.append(self.vocab.get(\"<unk>\", 0))\n",
        "          i += 1\n",
        "      tokens.append(self.vocab.get(\" \", 1))\n",
        "    if tokens:\n",
        "      tokens.pop()\n",
        "    return tokens\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    token_ids = self.encode(text)\n",
        "    return [self.reverse_vocab[id] for id in token_ids]\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \"\"\n",
        "    for id in ids:\n",
        "      text += self.reverse_vocab.get(id, \"<unk>\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "Gt1FiKC7jJlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(\"tokenizer_1.json\")"
      ],
      "metadata": {
        "id": "Tw8HIpIpjThq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.offline"
      ],
      "metadata": {
        "id": "wbjNpiYxqw4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_dots(sentences_data, title, dims=[0, 1, 2]):\n",
        "  data = [\n",
        "    go.Scatter3d(\n",
        "      x=sentence_data[\"words\"][:, dims[0]],\n",
        "      y=sentence_data[\"words\"][:, dims[1]],\n",
        "      z=sentence_data[\"words\"][:, dims[2]],\n",
        "      mode=\"markers+text\",\n",
        "      marker=dict(\n",
        "        size=6,\n",
        "        color=sentence_data[\"color\"],\n",
        "      ),\n",
        "      text=sentence_data[\"labels\"],\n",
        "      hoverinfo=\"text\",\n",
        "    ) for sentence_data in sentences_data\n",
        "  ]\n",
        "\n",
        "  layout = go.Layout(\n",
        "    scene=dict(\n",
        "      xaxis_title=\"Sertlik\",\n",
        "      yaxis_title=\"Parlaklık\",\n",
        "      zaxis_title=\"Kırmızılık\",\n",
        "    ),\n",
        "    title=title,\n",
        "  )\n",
        "\n",
        "  fig = go.Figure(data=data, layout=layout)\n",
        "  plotly.offline.iplot(fig)"
      ],
      "metadata": {
        "id": "9Dd15E5Fqt9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"the capital of the united states\"\n",
        "target = \"capital of the united stetes is\""
      ],
      "metadata": {
        "id": "Wb1CIk6DjMAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = tokenizer.encode(data)\n",
        "\n",
        "len(ids), ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW1PzQDejRrO",
        "outputId": "9b64bfd4-f83c-4e63-d56a-977abf11b543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12, [0, 61, 1, 61, 2, 61, 0, 61, 3, 61, 4, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_t = tokenizer.encode(target)\n",
        "\n",
        "len(ids_t), ids_t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4WEXuwnjnVD",
        "outputId": "723b8ca5-73e5-4149-df94-3c7e71e6f854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, [1, 61, 2, 61, 0, 61, 3, 61, 58, 62, 62, 62, 62, 58, 61, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Context length, bir dil modelinin aynı anda işleyebildiği en uzun giriş + çıkış token toplamıdır."
      ],
      "metadata": {
        "id": "D0gWxZSIkcdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = 32 #Gpt-4o 128k"
      ],
      "metadata": {
        "id": "nDrdCT9wjBoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eğer Context length dolmazsa sonu \"<<c>ped>\" ile doldurulur"
      ],
      "metadata": {
        "id": "DfoH8JMDkhyh"
      }
    }
  ]
}