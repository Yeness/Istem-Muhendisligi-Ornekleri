{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch -Uq\n",
        "!pip install transformers -Uq\n",
        "!pip install tiktoken -Uq\n",
        "!pip install datasets -Uq\n",
        "!pip install matplotlib -Uq"
      ],
      "metadata": {
        "id": "TMtz7Fpb0X_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxiEf_LTw-JO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import plotly.graph_objects as go\n",
        "import plotly.offline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self, vocab_path):\n",
        "    with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
        "      self.vocab = json.load(f)\n",
        "      self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    tokens = []\n",
        "\n",
        "    for word in text.split():\n",
        "      i = 0\n",
        "      while i < len(word):\n",
        "        found_match = False\n",
        "        for j in range(len(word), i, -1):\n",
        "          subword = word[i:j]\n",
        "          if subword in self.vocab:\n",
        "            tokens.append(self.vocab[subword])\n",
        "            i = j\n",
        "            found_match = True\n",
        "            break\n",
        "        if not found_match:\n",
        "          tokens.append(self.vocab.get(\"<unk>\", 0))\n",
        "          i += 1\n",
        "      tokens.append(self.vocab.get(\" \", 1))\n",
        "    if tokens:\n",
        "      tokens.pop()\n",
        "    return tokens\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    token_ids = self.encode(text)\n",
        "    return [self.reverse_vocab[id] for id in token_ids]\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \"\"\n",
        "    for id in ids:\n",
        "      text += self.reverse_vocab.get(id, \"<unk>\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "yjOk0RFLxXgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rotary_position_encoding(input: torch.Tensor, base=10000, device=\"cpu\"):\n",
        "  context_length, dimension = input.shape\n",
        "\n",
        "  assert dimension % 2 == 0, \"boyutlar eşit olmalıdır\"\n",
        "\n",
        "  half_dimension = dimension // 2\n",
        "\n",
        "  freqs_indices = torch.arange(0, half_dimension, device=device, dtype=torch.float32)\n",
        "  freqs = 1.0 / (base ** (freqs_indices / dimension))\n",
        "\n",
        "  positions = torch.arange(0, context_length, device=device, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "  angles = positions * freqs\n",
        "\n",
        "  sin_angles = torch.sin(angles)\n",
        "  cos_angles = torch.cos(angles)\n",
        "\n",
        "  input_even = input[:, :dimension // 2]\n",
        "  input_odd = input[:, dimension // 2:]\n",
        "\n",
        "  input_even_rotated = input_even * cos_angles - input_odd * sin_angles\n",
        "  input_odd_rotated = input_even * sin_angles + input_odd * cos_angles\n",
        "\n",
        "  input_rotated = torch.empty_like(input)\n",
        "\n",
        "  input_rotated[:, :dimension // 2] = input_even_rotated\n",
        "  input_rotated[:, dimension // 2:] = input_odd_rotated\n",
        "\n",
        "  return input_rotated"
      ],
      "metadata": {
        "id": "M696kvFbyqXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UstaModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, context_length):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    self.get_pos = get_rotary_position_encoding\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x = self.get_pos(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "mSeqspJMxhfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_dots(sentences_data, title, dims=[0, 1, 2]):\n",
        "  data = [\n",
        "    go.Scatter3d(\n",
        "      x=sentence_data[\"words\"][:, dims[0]],\n",
        "      y=sentence_data[\"words\"][:, dims[1]],\n",
        "      z=sentence_data[\"words\"][:, dims[2]],\n",
        "      mode=\"markers+text\",\n",
        "      marker=dict(\n",
        "        size=6,\n",
        "        color=sentence_data[\"color\"],\n",
        "      ),\n",
        "      text=sentence_data[\"labels\"],\n",
        "      hoverinfo=\"text\",\n",
        "    ) for sentence_data in sentences_data\n",
        "  ]\n",
        "\n",
        "  layout = go.Layout(\n",
        "    scene=dict(\n",
        "      xaxis_title=\"Hardness\",\n",
        "      yaxis_title=\"Brightness\",\n",
        "      zaxis_title=\"Redness\",\n",
        "    ),\n",
        "    title=title,\n",
        "    width=1000,\n",
        "    height=1000,\n",
        "  )\n",
        "\n",
        "  fig = go.Figure(data=data, layout=layout)\n",
        "  plotly.offline.iplot(fig)"
      ],
      "metadata": {
        "id": "FdD84jrxVHbY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}