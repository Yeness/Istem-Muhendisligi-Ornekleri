{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch -Uq\n",
        "!pip install transformers -Uq\n",
        "!pip install tiktoken -Uq\n",
        "!pip install datasets -Uq\n",
        "!pip install matplotlib -Uq"
      ],
      "metadata": {
        "id": "TMtz7Fpb0X_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxiEf_LTw-JO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import plotly.graph_objects as go\n",
        "import plotly.offline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self, vocab_path):\n",
        "    with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
        "      self.vocab = json.load(f)\n",
        "      self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    tokens = []\n",
        "\n",
        "    for word in text.split():\n",
        "      i = 0\n",
        "      while i < len(word):\n",
        "        found_match = False\n",
        "        for j in range(len(word), i, -1):\n",
        "          subword = word[i:j]\n",
        "          if subword in self.vocab:\n",
        "            tokens.append(self.vocab[subword])\n",
        "            i = j\n",
        "            found_match = True\n",
        "            break\n",
        "        if not found_match:\n",
        "          tokens.append(self.vocab.get(\"<unk>\", 0))\n",
        "          i += 1\n",
        "      tokens.append(self.vocab.get(\" \", 1))\n",
        "    if tokens:\n",
        "      tokens.pop()\n",
        "    return tokens\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    token_ids = self.encode(text)\n",
        "    return [self.reverse_vocab[id] for id in token_ids]\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \"\"\n",
        "    for id in ids:\n",
        "      text += self.reverse_vocab.get(id, \"<unk>\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "yjOk0RFLxXgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rotary_position_encoding(input: torch.Tensor, base=10000, device=\"cpu\"):\n",
        "  context_length, dimension = input.shape\n",
        "\n",
        "  assert dimension % 2 == 0, \"boyutlar eşit olmalıdır\"\n",
        "\n",
        "  half_dimension = dimension // 2\n",
        "\n",
        "  freqs_indices = torch.arange(0, half_dimension, device=device, dtype=torch.float32)\n",
        "  freqs = 1.0 / (base ** (freqs_indices / dimension))\n",
        "\n",
        "  positions = torch.arange(0, context_length, device=device, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "  angles = positions * freqs\n",
        "\n",
        "  sin_angles = torch.sin(angles)\n",
        "  cos_angles = torch.cos(angles)\n",
        "\n",
        "  input_even = input[:, :dimension // 2]\n",
        "  input_odd = input[:, dimension // 2:]\n",
        "\n",
        "  input_even_rotated = input_even * cos_angles - input_odd * sin_angles\n",
        "  input_odd_rotated = input_even * sin_angles + input_odd * cos_angles\n",
        "\n",
        "  input_rotated = torch.empty_like(input)\n",
        "\n",
        "  input_rotated[:, :dimension // 2] = input_even_rotated\n",
        "  input_rotated[:, dimension // 2:] = input_odd_rotated\n",
        "\n",
        "  return input_rotated"
      ],
      "metadata": {
        "id": "M696kvFbyqXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UstaModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, context_length):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    self.get_pos = get_rotary_position_encoding\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x = self.get_pos(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "mSeqspJMxhfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_dots(sentences_data, title, dims=[0, 1, 2]):\n",
        "  data = [\n",
        "    go.Scatter3d(\n",
        "      x=sentence_data[\"words\"][:, dims[0]],\n",
        "      y=sentence_data[\"words\"][:, dims[1]],\n",
        "      z=sentence_data[\"words\"][:, dims[2]],\n",
        "      mode=\"markers+text\",\n",
        "      marker=dict(\n",
        "        size=6,\n",
        "        color=sentence_data[\"color\"],\n",
        "      ),\n",
        "      text=sentence_data[\"labels\"],\n",
        "      hoverinfo=\"text\",\n",
        "    ) for sentence_data in sentences_data\n",
        "  ]\n",
        "\n",
        "  layout = go.Layout(\n",
        "    scene=dict(\n",
        "      xaxis_title=\"Hardness\",\n",
        "      yaxis_title=\"Brightness\",\n",
        "      zaxis_title=\"Redness\",\n",
        "    ),\n",
        "    title=title,\n",
        "    width=1000,\n",
        "    height=1000,\n",
        "  )\n",
        "\n",
        "  fig = go.Figure(data=data, layout=layout)\n",
        "  plotly.offline.iplot(fig)"
      ],
      "metadata": {
        "id": "FdD84jrxVHbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(\"tokenizer_1.json\")"
      ],
      "metadata": {
        "id": "BtJuWRmKzwBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"the capital of united states and the capital of france\""
      ],
      "metadata": {
        "id": "DzBbYhlz0S_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.encode(prompt)"
      ],
      "metadata": {
        "id": "9GFRXEsJ0U8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(tokens, dtype=torch.long)"
      ],
      "metadata": {
        "id": "eA-ncvrrTVUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "UlLZurtC0W2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UstaModel(vocab_size=len(tokenizer.vocab), embedding_dim=4, context_length=32)"
      ],
      "metadata": {
        "id": "8k7gKmra0aDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(x)"
      ],
      "metadata": {
        "id": "08NPz8ur0hNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_meanings = model(x)"
      ],
      "metadata": {
        "id": "vwxVkLGxtLzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_meanings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euTybVJFtXRq",
        "outputId": "75de9f24-5950-4801-fe3b-588477671ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "  {\n",
        "    \"words\": sentence_meanings.detach().numpy(),\n",
        "    \"labels\": tokenizer.tokenize(prompt),\n",
        "    \"color\": \"red\",\n",
        "  },\n",
        "]\n",
        "\n",
        "plot_dots(sentences, \"Models Context Space\")"
      ],
      "metadata": {
        "id": "3SxvafVHtcPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_meanings"
      ],
      "metadata": {
        "id": "_BW6ctPK1C35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_similarities = torch.zeros(sentence_meanings.shape[0], sentence_meanings.shape[0])\n",
        "for j in range(sentence_meanings.shape[0]):\n",
        "  j_similarities = torch.zeros(sentence_meanings.shape[0])\n",
        "\n",
        "  for i in range(sentence_meanings.shape[0]):\n",
        "    for k in range(sentence_meanings.shape[1]):\n",
        "      cs_j_i = sentence_meanings[j][k] * sentence_meanings[i][k]\n",
        "      j_similarities[i] += cs_j_i\n",
        "\n",
        "  all_similarities[j] = j_similarities"
      ],
      "metadata": {
        "id": "X-whiksWPSX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_weights = torch.softmax(all_similarities, dim=1)"
      ],
      "metadata": {
        "id": "TUVQx6SAPCrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_similarities.detach().numpy()"
      ],
      "metadata": {
        "id": "YGJH62C3PUij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''class UstaSelfAttention(nn.Module):\n",
        "  def __init__(self, embedding_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    self.q_weights = nn.Linear(embedding_dim, output_dim, bias=False)\n",
        "    self.k_weights = nn.Linear(embedding_dim, output_dim, bias=False)\n",
        "    self.v_weights = nn.Linear(embedding_dim, output_dim, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    q = self.q_weights(x)\n",
        "    k = self.k_weights(x)\n",
        "    v = self.v_weights(x)\n",
        "\n",
        "    attention_scores = q @ k.T\n",
        "    attention_weights = torch.softmax(attention_scores / k.shape[-1] ** 0.5, dim=1)\n",
        "    return attention_weights @ v'''"
      ],
      "metadata": {
        "id": "xFwINSa3yQN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''class UstaModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, context_length):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.pos_embedding = nn.Embedding(context_length, embedding_dim)\n",
        "    self.get_pos = get_rotary_position_encoding\n",
        "    self.self_attention = UstaSelfAttention(embedding_dim, embedding_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x = self.get_pos(x)\n",
        "    x = self.self_attention(x)\n",
        "    return x'''"
      ],
      "metadata": {
        "id": "vLGIF0fmyd_v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}